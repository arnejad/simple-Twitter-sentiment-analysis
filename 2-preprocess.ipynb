{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocessor as p\n",
    "import json\n",
    "import pandas as pd\n",
    "import csv\n",
    "import re #regular expression\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reformating the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next sections we are ging to transform the tweets from json to Pandas dataframe. We need to determin the column names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we just want to run an NLP task we are going to just use the text of each tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('db/tweets.json') as f:\n",
    "  tweets = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_list = []\n",
    "neg_list = []\n",
    "\n",
    "for i in range(1, len(tweets)):\n",
    "    if tweets[i][\"label\"] == 1 :\n",
    "        pos_list.append(p.clean(tweets[i][\"text\"]))\n",
    "    elif tweets[i][\"label\"] == 0:\n",
    "        neg_list.append(p.clean(tweets[i][\"text\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP tasks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Ashkan R.\n",
      "[nltk_data]     Nejad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import sent_tokenize, word_tokenize , RegexpTokenizer\n",
    "\n",
    "# incase of seeing unavailable package error uncomment section above\n",
    "\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "tmp = []\n",
    "for i in range(1, len(pos_list)):\n",
    "    tmp.extend(word_tokenize(pos_list[i]))\n",
    "    \n",
    "for i in range(1, len(neg_list)):\n",
    "    tmp.extend(word_tokenize(neg_list[i]))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filtering all the uncessary and duplicated symbols like . , ! (using ASCII code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Purple',\n",
       " 'EVEN',\n",
       " 'Upcoming',\n",
       " 'of',\n",
       " 'seeing',\n",
       " 'about',\n",
       " 'hear',\n",
       " 'Blue',\n",
       " 'deal',\n",
       " 'VIDEO',\n",
       " 'been',\n",
       " 'new',\n",
       " 'no',\n",
       " 'stabilizati…',\n",
       " 'fast',\n",
       " 'show',\n",
       " 'don',\n",
       " 'make',\n",
       " 'more',\n",
       " 'k',\n",
       " 'stepping',\n",
       " 'Workers',\n",
       " 'let',\n",
       " 'highest-end',\n",
       " 'NOT',\n",
       " 'Know',\n",
       " 'Go',\n",
       " 'this',\n",
       " 'listening',\n",
       " 'Says',\n",
       " 'screen',\n",
       " 'Adapters',\n",
       " 'model',\n",
       " 'if',\n",
       " 'need',\n",
       " 'phone',\n",
       " 'Earphone',\n",
       " 'trick',\n",
       " 'I',\n",
       " 'vs',\n",
       " 'means',\n",
       " 'available',\n",
       " 'OLED',\n",
       " 'box',\n",
       " 'the',\n",
       " 'upgrade',\n",
       " 'off',\n",
       " 'little',\n",
       " 'NEW',\n",
       " 'We',\n",
       " 'from',\n",
       " 'computer',\n",
       " 'Want',\n",
       " 'shit',\n",
       " 'Retweets',\n",
       " 'take',\n",
       " 'could',\n",
       " 'Feature',\n",
       " 'a…',\n",
       " 'Camera',\n",
       " 'classy',\n",
       " 'Self-isolate',\n",
       " 'pow…',\n",
       " 'junk',\n",
       " 'Strap',\n",
       " 's',\n",
       " 'can',\n",
       " 'connect',\n",
       " 'just',\n",
       " 'years',\n",
       " 'shift',\n",
       " 'Meet',\n",
       " 'Your',\n",
       " 'Sensor-Shift',\n",
       " 'cheat',\n",
       " 'do',\n",
       " 'AirPods',\n",
       " 'Apple',\n",
       " 'Ring',\n",
       " 'Pros',\n",
       " 'Under',\n",
       " 'Protective',\n",
       " 'end',\n",
       " 'Wit…',\n",
       " 'Production…',\n",
       " 'series',\n",
       " 'At',\n",
       " 'still',\n",
       " 'actually',\n",
       " 'regular',\n",
       " 'm',\n",
       " 'Lavanda',\n",
       " 'oled',\n",
       " 'resurrect',\n",
       " 'Make',\n",
       " 'eected',\n",
       " 'Now',\n",
       " 'Secured',\n",
       " 'Say',\n",
       " 'release',\n",
       " 'How',\n",
       " 'to',\n",
       " 'Case',\n",
       " 'inch',\n",
       " 'my',\n",
       " 't',\n",
       " 'Modd',\n",
       " 'are',\n",
       " 'spent',\n",
       " 'amp',\n",
       " 'KRCS',\n",
       " 'me',\n",
       " 'launch',\n",
       " 'cheapest',\n",
       " 'phones',\n",
       " 'Gen',\n",
       " 'tech',\n",
       " 'same',\n",
       " 'typing',\n",
       " 'stabilization',\n",
       " 'another',\n",
       " 'MacBook',\n",
       " 'game',\n",
       " 'also',\n",
       " 'some',\n",
       " 'Trooss',\n",
       " 'COOLER',\n",
       " 'rumour',\n",
       " 'at',\n",
       " 'witho…',\n",
       " 'Can',\n",
       " 'Mobile',\n",
       " 'around',\n",
       " 'is',\n",
       " 'big',\n",
       " 'since',\n",
       " 'again',\n",
       " 'Win',\n",
       " 'Periscope',\n",
       " 'buy',\n",
       " 'fall',\n",
       " 'change',\n",
       " 'a',\n",
       " 'This',\n",
       " 'top',\n",
       " 'After',\n",
       " 'Galaxy',\n",
       " 'both',\n",
       " 'we',\n",
       " 'want',\n",
       " 'sai…',\n",
       " 'machine',\n",
       " 'Soft',\n",
       " 'Cover',\n",
       " 'Silicone',\n",
       " 'only',\n",
       " 'one',\n",
       " 'Help',\n",
       " 'put',\n",
       " 'Mac\\xadBook',\n",
       " 'be',\n",
       " 'good',\n",
       " 'gt',\n",
       " 'Airpods',\n",
       " 'accept',\n",
       " 'feature',\n",
       " 'sale',\n",
       " 'and',\n",
       " 'camera',\n",
       " 'And',\n",
       " 'something',\n",
       " 'inadvertently',\n",
       " 'out',\n",
       " 'Could',\n",
       " 'depth',\n",
       " 'Side',\n",
       " 'Is',\n",
       " 'speaker',\n",
       " 'Did',\n",
       " 'store',\n",
       " 'economy',\n",
       " 'For',\n",
       " 'reduced',\n",
       " 'away',\n",
       " 'sensor-shift',\n",
       " 'them',\n",
       " 'Lens…',\n",
       " 'Max',\n",
       " '``',\n",
       " 'Best',\n",
       " 'warranty',\n",
       " 'Foxconn',\n",
       " 'think',\n",
       " 'support',\n",
       " 'USED-',\n",
       " 'll',\n",
       " 'in',\n",
       " 'products',\n",
       " 'all',\n",
       " 'iPhone',\n",
       " 'it',\n",
       " 'Do',\n",
       " 'you',\n",
       " 'started',\n",
       " 'she',\n",
       " 'see',\n",
       " 'appeal',\n",
       " 'automatically',\n",
       " 'All',\n",
       " 'know',\n",
       " 'but',\n",
       " 'sensors',\n",
       " 'will',\n",
       " 'customers',\n",
       " 'MSRP',\n",
       " 'pair',\n",
       " 'chances',\n",
       " 'lEAK',\n",
       " 'Border',\n",
       " 'D',\n",
       " 'Enough',\n",
       " 'lucky',\n",
       " 'More',\n",
       " 'Bag',\n",
       " 'return-',\n",
       " 'hit',\n",
       " 'loose',\n",
       " 'Earbuds',\n",
       " 'gen',\n",
       " 'It',\n",
       " 'they',\n",
       " 'image',\n",
       " 'that',\n",
       " 'Find',\n",
       " 'return',\n",
       " 'come',\n",
       " 'Premium',\n",
       " 'not',\n",
       " 'https',\n",
       " 'name',\n",
       " 'this.…',\n",
       " 'followers',\n",
       " 'online',\n",
       " 'giving',\n",
       " 'up',\n",
       " 'exclusively',\n",
       " 'Varient',\n",
       " 'S20',\n",
       " 'might',\n",
       " 'once',\n",
       " 'bring',\n",
       " 'cont',\n",
       " 'enter',\n",
       " 'Ming-Chi',\n",
       " 'The',\n",
       " 'push-off',\n",
       " 'benefits',\n",
       " 'Zoom',\n",
       " 'Airpower',\n",
       " 'shipments',\n",
       " 'time',\n",
       " 'two',\n",
       " 'As',\n",
       " 'later',\n",
       " 'Please',\n",
       " 'isnt',\n",
       " 'today',\n",
       " 'Birthday',\n",
       " 'for',\n",
       " 'now',\n",
       " 'Brand',\n",
       " 'reveals',\n",
       " 'best',\n",
       " 'Samsung',\n",
       " 'Stabilization',\n",
       " 'HDMI',\n",
       " 'washing',\n",
       " 'what',\n",
       " 'get',\n",
       " 'please',\n",
       " 'Pro',\n",
       " 'Who',\n",
       " 'ports',\n",
       " 'comes',\n",
       " 'details',\n",
       " 'Navy',\n",
       " 'New',\n",
       " 'beat',\n",
       " 'in…',\n",
       " 'Cartoon',\n",
       " 'going',\n",
       " 'Has',\n",
       " 'Color',\n",
       " 'starting',\n",
       " 'One',\n",
       " 'doesn',\n",
       " 'morning',\n",
       " 'hold',\n",
       " 'buying',\n",
       " 'through',\n",
       " 'Update',\n",
       " 'fuck',\n",
       " 'next',\n",
       " 'Sooooo',\n",
       " 'launched',\n",
       " 'i…',\n",
       " 'hate',\n",
       " 'iphone',\n",
       " 'Price',\n",
       " 'Insta',\n",
       " 'given',\n",
       " 'on',\n",
       " 'Kuo…',\n",
       " 'your',\n",
       " 'type',\n",
       " 'work',\n",
       " 'leak',\n",
       " 'sensor',\n",
       " 'You',\n",
       " 'production',\n",
       " 'would',\n",
       " 'Cute',\n",
       " 'Demand',\n",
       " 'stabilisation',\n",
       " 'Here',\n",
       " 'these',\n",
       " 'turn',\n",
       " 'winners',\n",
       " 'using',\n",
       " 'with',\n",
       " 'versions']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = []\n",
    "for i in range(1, len(tmp)):\n",
    "    if 64 <ord(tmp[i][0])< 123:\n",
    "            corpus.append(tmp[i])\n",
    "\n",
    "corpus = set(corpus)\n",
    "corpus = list(corpus)\n",
    "\n",
    "len(corpus)\n",
    "tokenized_text= [sentence.split() for sentence in corpus]\n",
    "\n",
    "corpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Bag-of-words\n",
    "\n",
    "The data is going to be saved in a dataframe in the spark-friendly format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as numpy\n",
    "\n",
    "# feats = []\n",
    "df = pd.DataFrame(columns=['features', 'label'])\n",
    "\n",
    "for sentence in pos_list:\n",
    "    words = word_tokenize(sentence)  \n",
    "    bag_vector = numpy.zeros(len(corpus))\n",
    "    for word in words:\n",
    "        for i,w in enumerate(corpus):\n",
    "            if word == w:\n",
    "#                 print(word, w, i, corpus[i])                \n",
    "                bag_vector[i] += 1 \n",
    "\n",
    "#     feats.append(bag_vector)\n",
    "    df = df.append({'features': bag_vector, 'label': 1}, ignore_index=True)\n",
    "    \n",
    "for sentence in neg_list:\n",
    "    words = word_tokenize(sentence)  \n",
    "    bag_vector = numpy.zeros(len(corpus))\n",
    "    for word in words:\n",
    "        for i,w in enumerate(corpus):\n",
    "            if word == w:\n",
    "                bag_vector[i] += 1 \n",
    "#     feats.append(bag_vector)\n",
    "    df = df.append({'features': bag_vector, 'label': 0}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving data into csv for later uses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open(\"feats.csv\",\"w+\") as my_csv:\n",
    "    csvWriter = csv.writer(my_csv,delimiter=',')\n",
    "    csvWriter.writerows(feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have all our tweet texts vectorized and ready for machine learning process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Used links:\n",
    "\n",
    "[1] https://www.freecodecamp.org/news/an-introduction-to-bag-of-words-and-how-to-code-it-in-python-for-nlp-282e87a9da04/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
